Not using distributed mode
[12:48:14.935947] job dir: /app/src/fsfm-3c/finuetune/cross_dataset_unseen_DiFF
[12:48:14.935999] Namespace(batch_size=4,
epochs=1,
accum_iter=1,
model='vit_small_patch16',
input_size=224,
normalize_from_IMN=False,
apply_simple_augment=False,
drop_path=0.1,
clip_grad=None,
weight_decay=0.05,
lr=None,
blr=0.001,
layer_decay=0.75,
min_lr=1e-06,
warmup_epochs=5,
color_jitter=None,
aa='rand-m9-mstd0.5-inc1',
smoothing=0.1,
reprob=0.25,
remode='pixel',
recount=1,
resplit=False,
mixup=0,
cutmix=0,
cutmix_minmax=None,
mixup_prob=1.0,
mixup_switch_prob=0.5,
mixup_mode='batch',
finetune='',
global_pool=True,
input_dir='/app/datasets/finetune/DiFF/set_1_lfw_mock',
nb_classes=2,
output_dir='./src/fsfm-3c/finuetune/cross_dataset_unseen_DiFF/output_finetune_set_1_lfw_mock/',
log_dir='./src/fsfm-3c/finuetune/cross_dataset_unseen_DiFF/output_finetune_set_1_lfw_mock/',
device='cpu',
seed=0,
resume='',
start_epoch=0,
eval=False,
dist_eval=False,
num_workers=0,
pin_mem=True,
world_size=1,
local_rank=-1,
dist_on_itp=False,
dist_url='env://',
distributed=False)
[12:48:14.939574] Warning: Could not load dataset stats (Stats file not found: ./src/fsfm-3c/finuetune/cross_dataset_unseen_DiFF/output_finetune_set_1_lfw_mock/pretrain_ds_mean_std.txt). Using VGGFace2 values instead.
[12:48:14.940731] Dataset ImageFolder
    Number of datapoints: 8
    Root location: /app/datasets/finetune/DiFF/set_1_lfw_mock/train
    StandardTransform
Transform: Compose(
               Resize(size=(224, 224), interpolation=bicubic, max_size=None, antialias=True)
               ToTensor()
               Normalize(mean=[0.5482207536697388, 0.42340534925460815, 0.3654651641845703], std=[0.2789176106452942, 0.2438540756702423, 0.23493893444538116])
           )
[12:48:14.940821] Warning: Could not load dataset stats (Stats file not found: ./src/fsfm-3c/finuetune/cross_dataset_unseen_DiFF/output_finetune_set_1_lfw_mock/pretrain_ds_mean_std.txt). Using VGGFace2 values instead.
[12:48:14.941437] Dataset ImageFolder
    Number of datapoints: 8
    Root location: /app/datasets/finetune/DiFF/set_1_lfw_mock/val
    StandardTransform
Transform: Compose(
               Resize(size=(224, 224), interpolation=bicubic, max_size=None, antialias=True)
               ToTensor()
               Normalize(mean=[0.5482207536697388, 0.42340534925460815, 0.3654651641845703], std=[0.2789176106452942, 0.2438540756702423, 0.23493893444538116])
           )
[12:48:14.941505] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x701ff97c89d0>
[12:48:14.941536] [INFO]log dir: %./src/fsfm-3c/finuetune/cross_dataset_unseen_DiFF/output_finetune_set_1_lfw_mock/
[12:48:15.592848] ----------------------------------------------------------------
[12:48:15.592873]         Layer (type)               Output Shape         Param #
[12:48:15.592876] ================================================================
[12:48:15.593800]             Conv2d-1          [-1, 384, 14, 14]         295,296
[12:48:15.593846]         PatchEmbed-2             [-1, 196, 384]               0
[12:48:15.593858]            Dropout-3             [-1, 197, 384]               0
[12:48:15.593877]          LayerNorm-4             [-1, 197, 384]             768
[12:48:15.593889]             Linear-5            [-1, 197, 1152]         443,520
[12:48:15.593900]            Dropout-6         [-1, 12, 197, 197]               0
[12:48:15.593910]             Linear-7             [-1, 197, 384]         147,840
[12:48:15.593919]            Dropout-8             [-1, 197, 384]               0
[12:48:15.593969]          Attention-9             [-1, 197, 384]               0
[12:48:15.593978]          Identity-10             [-1, 197, 384]               0
[12:48:15.593988]         LayerNorm-11             [-1, 197, 384]             768
[12:48:15.593999]            Linear-12            [-1, 197, 1536]         591,360
[12:48:15.594008]              GELU-13            [-1, 197, 1536]               0
[12:48:15.594017]           Dropout-14            [-1, 197, 1536]               0
[12:48:15.594027]            Linear-15             [-1, 197, 384]         590,208
[12:48:15.594036]           Dropout-16             [-1, 197, 384]               0
[12:48:15.594050]               Mlp-17             [-1, 197, 384]               0
[12:48:15.594059]          Identity-18             [-1, 197, 384]               0
[12:48:15.594068]             Block-19             [-1, 197, 384]               0
[12:48:15.594078]         LayerNorm-20             [-1, 197, 384]             768
[12:48:15.594088]            Linear-21            [-1, 197, 1152]         443,520
[12:48:15.594097]           Dropout-22         [-1, 12, 197, 197]               0
[12:48:15.594107]            Linear-23             [-1, 197, 384]         147,840
[12:48:15.594116]           Dropout-24             [-1, 197, 384]               0
[12:48:15.594126]         Attention-25             [-1, 197, 384]               0
[12:48:15.594135]          DropPath-26             [-1, 197, 384]               0
[12:48:15.594146]         LayerNorm-27             [-1, 197, 384]             768
[12:48:15.594156]            Linear-28            [-1, 197, 1536]         591,360
[12:48:15.594165]              GELU-29            [-1, 197, 1536]               0
[12:48:15.594173]           Dropout-30            [-1, 197, 1536]               0
[12:48:15.594183]            Linear-31             [-1, 197, 384]         590,208
[12:48:15.594192]           Dropout-32             [-1, 197, 384]               0
[12:48:15.594201]               Mlp-33             [-1, 197, 384]               0
[12:48:15.594209]          DropPath-34             [-1, 197, 384]               0
[12:48:15.594218]             Block-35             [-1, 197, 384]               0
[12:48:15.594227]         LayerNorm-36             [-1, 197, 384]             768
[12:48:15.594237]            Linear-37            [-1, 197, 1152]         443,520
[12:48:15.594246]           Dropout-38         [-1, 12, 197, 197]               0
[12:48:15.594256]            Linear-39             [-1, 197, 384]         147,840
[12:48:15.594265]           Dropout-40             [-1, 197, 384]               0
[12:48:15.594274]         Attention-41             [-1, 197, 384]               0
[12:48:15.594282]          DropPath-42             [-1, 197, 384]               0
[12:48:15.594292]         LayerNorm-43             [-1, 197, 384]             768
[12:48:15.594302]            Linear-44            [-1, 197, 1536]         591,360
[12:48:15.594311]              GELU-45            [-1, 197, 1536]               0
[12:48:15.594319]           Dropout-46            [-1, 197, 1536]               0
[12:48:15.594330]            Linear-47             [-1, 197, 384]         590,208
[12:48:15.594339]           Dropout-48             [-1, 197, 384]               0
[12:48:15.594348]               Mlp-49             [-1, 197, 384]               0
[12:48:15.594356]          DropPath-50             [-1, 197, 384]               0
[12:48:15.594365]             Block-51             [-1, 197, 384]               0
[12:48:15.594375]         LayerNorm-52             [-1, 197, 384]             768
[12:48:15.594385]            Linear-53            [-1, 197, 1152]         443,520
[12:48:15.594394]           Dropout-54         [-1, 12, 197, 197]               0
[12:48:15.594404]            Linear-55             [-1, 197, 384]         147,840
[12:48:15.594412]           Dropout-56             [-1, 197, 384]               0
[12:48:15.594422]         Attention-57             [-1, 197, 384]               0
[12:48:15.594430]          DropPath-58             [-1, 197, 384]               0
[12:48:15.594440]         LayerNorm-59             [-1, 197, 384]             768
[12:48:15.594450]            Linear-60            [-1, 197, 1536]         591,360
[12:48:15.594459]              GELU-61            [-1, 197, 1536]               0
[12:48:15.594467]           Dropout-62            [-1, 197, 1536]               0
[12:48:15.594477]            Linear-63             [-1, 197, 384]         590,208
[12:48:15.594486]           Dropout-64             [-1, 197, 384]               0
[12:48:15.594494]               Mlp-65             [-1, 197, 384]               0
[12:48:15.594607]          DropPath-66             [-1, 197, 384]               0
[12:48:15.594620]             Block-67             [-1, 197, 384]               0
[12:48:15.594635]         LayerNorm-68             [-1, 197, 384]             768
[12:48:15.594645]            Linear-69            [-1, 197, 1152]         443,520
[12:48:15.594655]           Dropout-70         [-1, 12, 197, 197]               0
[12:48:15.594666]            Linear-71             [-1, 197, 384]         147,840
[12:48:15.594676]           Dropout-72             [-1, 197, 384]               0
[12:48:15.594685]         Attention-73             [-1, 197, 384]               0
[12:48:15.594694]          DropPath-74             [-1, 197, 384]               0
[12:48:15.594704]         LayerNorm-75             [-1, 197, 384]             768
[12:48:15.594715]            Linear-76            [-1, 197, 1536]         591,360
[12:48:15.594724]              GELU-77            [-1, 197, 1536]               0
[12:48:15.594733]           Dropout-78            [-1, 197, 1536]               0
[12:48:15.594743]            Linear-79             [-1, 197, 384]         590,208
[12:48:15.594751]           Dropout-80             [-1, 197, 384]               0
[12:48:15.594760]               Mlp-81             [-1, 197, 384]               0
[12:48:15.594769]          DropPath-82             [-1, 197, 384]               0
[12:48:15.594778]             Block-83             [-1, 197, 384]               0
[12:48:15.594788]         LayerNorm-84             [-1, 197, 384]             768
[12:48:15.594799]            Linear-85            [-1, 197, 1152]         443,520
[12:48:15.594807]           Dropout-86         [-1, 12, 197, 197]               0
[12:48:15.594817]            Linear-87             [-1, 197, 384]         147,840
[12:48:15.594826]           Dropout-88             [-1, 197, 384]               0
[12:48:15.594835]         Attention-89             [-1, 197, 384]               0
[12:48:15.594844]          DropPath-90             [-1, 197, 384]               0
[12:48:15.594854]         LayerNorm-91             [-1, 197, 384]             768
[12:48:15.594864]            Linear-92            [-1, 197, 1536]         591,360
[12:48:15.594873]              GELU-93            [-1, 197, 1536]               0
[12:48:15.594882]           Dropout-94            [-1, 197, 1536]               0
[12:48:15.594892]            Linear-95             [-1, 197, 384]         590,208
[12:48:15.594901]           Dropout-96             [-1, 197, 384]               0
[12:48:15.594909]               Mlp-97             [-1, 197, 384]               0
[12:48:15.594918]          DropPath-98             [-1, 197, 384]               0
[12:48:15.594927]             Block-99             [-1, 197, 384]               0
[12:48:15.594938]        LayerNorm-100             [-1, 197, 384]             768
[12:48:15.594948]           Linear-101            [-1, 197, 1152]         443,520
[12:48:15.594957]          Dropout-102         [-1, 12, 197, 197]               0
[12:48:15.594968]           Linear-103             [-1, 197, 384]         147,840
[12:48:15.594976]          Dropout-104             [-1, 197, 384]               0
[12:48:15.594985]        Attention-105             [-1, 197, 384]               0
[12:48:15.594994]         DropPath-106             [-1, 197, 384]               0
[12:48:15.595004]        LayerNorm-107             [-1, 197, 384]             768
[12:48:15.595013]           Linear-108            [-1, 197, 1536]         591,360
[12:48:15.595022]             GELU-109            [-1, 197, 1536]               0
[12:48:15.595032]          Dropout-110            [-1, 197, 1536]               0
[12:48:15.595041]           Linear-111             [-1, 197, 384]         590,208
[12:48:15.595050]          Dropout-112             [-1, 197, 384]               0
[12:48:15.595059]              Mlp-113             [-1, 197, 384]               0
[12:48:15.595068]         DropPath-114             [-1, 197, 384]               0
[12:48:15.595077]            Block-115             [-1, 197, 384]               0
[12:48:15.595087]        LayerNorm-116             [-1, 197, 384]             768
[12:48:15.595097]           Linear-117            [-1, 197, 1152]         443,520
[12:48:15.595106]          Dropout-118         [-1, 12, 197, 197]               0
[12:48:15.595116]           Linear-119             [-1, 197, 384]         147,840
[12:48:15.595124]          Dropout-120             [-1, 197, 384]               0
[12:48:15.595133]        Attention-121             [-1, 197, 384]               0
[12:48:15.595142]         DropPath-122             [-1, 197, 384]               0
[12:48:15.595152]        LayerNorm-123             [-1, 197, 384]             768
[12:48:15.595162]           Linear-124            [-1, 197, 1536]         591,360
[12:48:15.595170]             GELU-125            [-1, 197, 1536]               0
[12:48:15.595179]          Dropout-126            [-1, 197, 1536]               0
[12:48:15.595189]           Linear-127             [-1, 197, 384]         590,208
[12:48:15.595197]          Dropout-128             [-1, 197, 384]               0
[12:48:15.595206]              Mlp-129             [-1, 197, 384]               0
[12:48:15.595215]         DropPath-130             [-1, 197, 384]               0
[12:48:15.595224]            Block-131             [-1, 197, 384]               0
[12:48:15.595234]        LayerNorm-132             [-1, 197, 384]             768
[12:48:15.595244]           Linear-133            [-1, 197, 1152]         443,520
[12:48:15.595253]          Dropout-134         [-1, 12, 197, 197]               0
[12:48:15.595263]           Linear-135             [-1, 197, 384]         147,840
[12:48:15.595272]          Dropout-136             [-1, 197, 384]               0
[12:48:15.595280]        Attention-137             [-1, 197, 384]               0
[12:48:15.595289]         DropPath-138             [-1, 197, 384]               0
[12:48:15.595299]        LayerNorm-139             [-1, 197, 384]             768
[12:48:15.595309]           Linear-140            [-1, 197, 1536]         591,360
[12:48:15.595318]             GELU-141            [-1, 197, 1536]               0
[12:48:15.595326]          Dropout-142            [-1, 197, 1536]               0
[12:48:15.595337]           Linear-143             [-1, 197, 384]         590,208
[12:48:15.595345]          Dropout-144             [-1, 197, 384]               0
[12:48:15.595354]              Mlp-145             [-1, 197, 384]               0
[12:48:15.595363]         DropPath-146             [-1, 197, 384]               0
[12:48:15.595372]            Block-147             [-1, 197, 384]               0
[12:48:15.595382]        LayerNorm-148             [-1, 197, 384]             768
[12:48:15.595392]           Linear-149            [-1, 197, 1152]         443,520
[12:48:15.595400]          Dropout-150         [-1, 12, 197, 197]               0
[12:48:15.595410]           Linear-151             [-1, 197, 384]         147,840
[12:48:15.595419]          Dropout-152             [-1, 197, 384]               0
[12:48:15.595428]        Attention-153             [-1, 197, 384]               0
[12:48:15.595437]         DropPath-154             [-1, 197, 384]               0
[12:48:15.595447]        LayerNorm-155             [-1, 197, 384]             768
[12:48:15.595457]           Linear-156            [-1, 197, 1536]         591,360
[12:48:15.595465]             GELU-157            [-1, 197, 1536]               0
[12:48:15.595474]          Dropout-158            [-1, 197, 1536]               0
[12:48:15.595484]           Linear-159             [-1, 197, 384]         590,208
[12:48:15.595493]          Dropout-160             [-1, 197, 384]               0
[12:48:15.595501]              Mlp-161             [-1, 197, 384]               0
[12:48:15.595510]         DropPath-162             [-1, 197, 384]               0
[12:48:15.595519]            Block-163             [-1, 197, 384]               0
[12:48:15.595530] /app/src/fsfm-3c/util/misc.py:259: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self._scaler = torch.cuda.amp.GradScaler()
/usr/local/lib/python3.9/site-packages/torch/amp/grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.
  warnings.warn(
/usr/local/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.
  warnings.warn(
/usr/local/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.
  warnings.warn(
       LayerNorm-164             [-1, 197, 384]             768
[12:48:15.595585]           Linear-165            [-1, 197, 1152]         443,520
[12:48:15.595600]          Dropout-166         [-1, 12, 197, 197]               0
[12:48:15.595611]           Linear-167             [-1, 197, 384]         147,840
[12:48:15.595621]          Dropout-168             [-1, 197, 384]               0
[12:48:15.595648]        Attention-169             [-1, 197, 384]               0
[12:48:15.595667]         DropPath-170             [-1, 197, 384]               0
[12:48:15.595683]        LayerNorm-171             [-1, 197, 384]             768
[12:48:15.595695]           Linear-172            [-1, 197, 1536]         591,360
[12:48:15.595705]             GELU-173            [-1, 197, 1536]               0
[12:48:15.595715]          Dropout-174            [-1, 197, 1536]               0
[12:48:15.595725]           Linear-175             [-1, 197, 384]         590,208
[12:48:15.595734]          Dropout-176             [-1, 197, 384]               0
[12:48:15.595743]              Mlp-177             [-1, 197, 384]               0
[12:48:15.595752]         DropPath-178             [-1, 197, 384]               0
[12:48:15.595761]            Block-179             [-1, 197, 384]               0
[12:48:15.595771]        LayerNorm-180             [-1, 197, 384]             768
[12:48:15.595781]           Linear-181            [-1, 197, 1152]         443,520
[12:48:15.595790]          Dropout-182         [-1, 12, 197, 197]               0
[12:48:15.595800]           Linear-183             [-1, 197, 384]         147,840
[12:48:15.595809]          Dropout-184             [-1, 197, 384]               0
[12:48:15.595818]        Attention-185             [-1, 197, 384]               0
[12:48:15.595827]         DropPath-186             [-1, 197, 384]               0
[12:48:15.595837]        LayerNorm-187             [-1, 197, 384]             768
[12:48:15.595847]           Linear-188            [-1, 197, 1536]         591,360
[12:48:15.595856]             GELU-189            [-1, 197, 1536]               0
[12:48:15.595865]          Dropout-190            [-1, 197, 1536]               0
[12:48:15.595875]           Linear-191             [-1, 197, 384]         590,208
[12:48:15.595884]          Dropout-192             [-1, 197, 384]               0
[12:48:15.595893]              Mlp-193             [-1, 197, 384]               0
[12:48:15.595902]         DropPath-194             [-1, 197, 384]               0
[12:48:15.595910]            Block-195             [-1, 197, 384]               0
[12:48:15.595921]        LayerNorm-196                  [-1, 384]             768
[12:48:15.595931]           Linear-197                    [-1, 2]             770
[12:48:15.595996] ================================================================
[12:48:15.596002] Total params: 21,590,402
[12:48:15.596005] Trainable params: 21,590,402
[12:48:15.596038] Non-trainable params: 0
[12:48:15.596042] ----------------------------------------------------------------
[12:48:15.596047] Input size (MB): 0.57
[12:48:15.596050] Forward/backward pass size (MB): 224.44
[12:48:15.596052] Params size (MB): 82.36
[12:48:15.596055] Estimated Total Size (MB): 307.37
[12:48:15.596057] ----------------------------------------------------------------
[12:48:15.596174] None
[12:48:15.596985] Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1-11): 11 x Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (pre_logits): Identity()
  (head): Linear(in_features=384, out_features=2, bias=True)
  (fc_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
)
[12:48:15.596992] number of params (M): 21.67
[12:48:15.596999] base lr: 1.00e-03
[12:48:15.597002] actual lr: 1.56e-05
[12:48:15.597005] accumulate grad iterations: 1
[12:48:15.597007] effective batch size: 4
[12:48:15.600496] criterion = LabelSmoothingCrossEntropy()
[12:48:15.600596] Start training for 1 epochs
[12:48:15.601847] log_dir: ./src/fsfm-3c/finuetune/cross_dataset_unseen_DiFF/output_finetune_set_1_lfw_mock/
[12:48:46.259908] Epoch: [0]  [0/2]  eta: 0:01:01  lr: 0.000000  loss: 0.8945 (0.8945)  time: 30.6563  data: 0.0281
[12:49:22.671769] Epoch: [0]  [1/2]  eta: 0:00:33  lr: 0.000002  loss: 0.6680 (0.7812)  time: 33.5335  data: 0.0202
[12:49:22.671921] Epoch: [0] Total time: 0:01:07 (33.5350 s / it)
[12:49:22.671935] Averaged stats: lr: 0.000002  loss: 0.6680 (0.7812)
[12:49:35.632419] Test:  [0/2]  eta: 0:00:25  loss: 0.3374 (0.3374)  acc: 100.0000 (100.0000)  auc: nan (nan)  time: 12.9585  data: 0.0144
[12:49:48.454339] Test:  [1/2]  eta: 0:00:12  loss: 0.3374 (0.7814)  acc: 0.0000 (50.0000)  auc: nan (nan)  time: 12.8901  data: 0.0135
[12:49:48.454513] Test: Total time: 0:00:25 (12.8905 s / it)
[12:49:48.454537] * Acc 50.000 Auc nan  loss 0.781
[12:49:48.454571] AUC of the network on the 8 val images: nan%
[12:49:48.454577] Max auc: 0.00%
[12:49:48.860228] Save model with min_val_loss at epoch: 0
[12:49:46.899399] Training time 0:01:31
